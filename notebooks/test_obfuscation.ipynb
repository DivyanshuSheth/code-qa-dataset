{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")\n",
    "# you can use any of the following config names as a second argument:\n",
    "# \"all\", \"go\", \"java\", \"javascript\", \n",
    "# \"php\", \"python\", \"ruby\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################  0  ################################\n",
      "def export_ruptures_csv(ekey, dstore):\n",
      "    \"\"\"\n",
      "    :param ekey: export key, i.e. a pair (datastore key, fmt)\n",
      "    :param dstore: datastore object\n",
      "    \"\"\"\n",
      "    oq = dstore['oqparam']\n",
      "    if 'scenario' in oq.calculation_mode:\n",
      "        return []\n",
      "    dest = dstore.export_path('ruptures.csv')\n",
      "    header = ('rupid multiplicity mag centroid_lon centroid_lat '\n",
      "              'centroid_depth trt strike dip rake boundary').split()\n",
      "    rows = []\n",
      "    for rgetter in gen_rupture_getters(dstore):\n",
      "        rups = rgetter.get_ruptures()\n",
      "        rup_data = calc.RuptureData(rgetter.trt, rgetter.rlzs_by_gsim)\n",
      "        for r in rup_data.to_array(rups):\n",
      "            rows.append(\n",
      "                (r['rup_id'], r['multiplicity'], r['mag'],\n",
      "                 r['lon'], r['lat'], r['depth'],\n",
      "                 rgetter.trt, r['strike'], r['dip'], r['rake'],\n",
      "                 r['boundary']))\n",
      "    rows.sort()  # by rupture serial\n",
      "    comment = 'investigation_time=%s, ses_per_logic_tree_path=%s' % (\n",
      "        oq.investigation_time, oq.ses_per_logic_tree_path)\n",
      "    writers.write_csv(dest, rows, header=header, sep='\\t', comment=comment)\n",
      "    return [dest]\n",
      "\n",
      "################################  1  ################################\n",
      "def export_hmaps_csv(key, dest, sitemesh, array, comment):\n",
      "    \"\"\"\n",
      "    Export the hazard maps of the given realization into CSV.\n",
      "\n",
      "    :param key: output_type and export_type\n",
      "    :param dest: name of the exported file\n",
      "    :param sitemesh: site collection\n",
      "    :param array: a composite array of dtype hmap_dt\n",
      "    :param comment: comment to use as header of the exported CSV file\n",
      "    \"\"\"\n",
      "    curves = util.compose_arrays(sitemesh, array)\n",
      "    writers.write_csv(dest, curves, comment=comment)\n",
      "    return [dest]\n",
      "\n",
      "################################  2  ################################\n",
      "def add_imt(fname, imt):\n",
      "    \"\"\"\n",
      "    >>> add_imt('/path/to/hcurve_23.csv', 'SA(0.1)')\n",
      "    '/path/to/hcurve-SA(0.1)_23.csv'\n",
      "    \"\"\"\n",
      "    name = os.path.basename(fname)\n",
      "    newname = re.sub(r'(_\\d+\\.)', '-%s\\\\1' % imt, name)\n",
      "    return os.path.join(os.path.dirname(fname), newname)\n",
      "\n",
      "################################  3  ################################\n",
      "def export_hcurves_by_imt_csv(\n",
      "        key, kind, rlzs_assoc, fname, sitecol, array, oq, checksum):\n",
      "    \"\"\"\n",
      "    Export the curves of the given realization into CSV.\n",
      "\n",
      "    :param key: output_type and export_type\n",
      "    :param kind: a string with the kind of output (realization or statistics)\n",
      "    :param rlzs_assoc: a :class:`openquake.commonlib.source.RlzsAssoc` instance\n",
      "    :param fname: name of the exported file\n",
      "    :param sitecol: site collection\n",
      "    :param array: an array of shape (N, L) and dtype numpy.float32\n",
      "    :param oq: job.ini parameters\n",
      "    \"\"\"\n",
      "    nsites = len(sitecol)\n",
      "    fnames = []\n",
      "    for imt, imls in oq.imtls.items():\n",
      "        slc = oq.imtls(imt)\n",
      "        dest = add_imt(fname, imt)\n",
      "        lst = [('lon', F32), ('lat', F32), ('depth', F32)]\n",
      "        for iml in imls:\n",
      "            lst.append(('poe-%s' % iml, F32))\n",
      "        hcurves = numpy.zeros(nsites, lst)\n",
      "        for sid, lon, lat, dep in zip(\n",
      "                range(nsites), sitecol.lons, sitecol.lats, sitecol.depths):\n",
      "            hcurves[sid] = (lon, lat, dep) + tuple(array[sid, slc])\n",
      "        fnames.append(writers.write_csv(dest, hcurves, comment=_comment(\n",
      "            rlzs_assoc, kind, oq.investigation_time) + (\n",
      "                ', imt=\"%s\", checksum=%d' % (imt, checksum)\n",
      "            ), header=[name for (name, dt) in lst]))\n",
      "    return fnames\n",
      "\n",
      "################################  4  ################################\n",
      "def hazard_curve_name(dstore, ekey, kind, rlzs_assoc):\n",
      "    \"\"\"\n",
      "    :param calc_id: the calculation ID\n",
      "    :param ekey: the export key\n",
      "    :param kind: the kind of key\n",
      "    :param rlzs_assoc: a RlzsAssoc instance\n",
      "    \"\"\"\n",
      "    key, fmt = ekey\n",
      "    prefix = {'hcurves': 'hazard_curve', 'hmaps': 'hazard_map',\n",
      "              'uhs': 'hazard_uhs'}[key]\n",
      "    if kind.startswith('quantile-'):  # strip the 7 characters 'hazard_'\n",
      "        fname = dstore.build_fname('quantile_' + prefix[7:], kind[9:], fmt)\n",
      "    else:\n",
      "        fname = dstore.build_fname(prefix, kind, fmt)\n",
      "    return fname\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"\\n################################ \", i, \" ################################\")\n",
    "    print(dataset[\"train\"][i][\"whole_func_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/data/datasets/models/hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 22:09:15.168558: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-10 22:09:15.212893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 22:09:15.212927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 22:09:15.214935: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 22:09:15.223468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 22:09:16.619091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7268f54c534f68bb341e8f01279e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", \n",
    "                model=\"deepseek-ai/deepseek-coder-7b-instruct-v1.5\", \n",
    "                device='cuda:0',\n",
    "                model_kwargs={'max_length': 1000, 'num_return_sequences': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:100015 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start of code> def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
      "    \"\"\"\n",
      "    Trains a k-nearest neighbors classifier for face recognition.\n",
      "\n",
      "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
      "\n",
      "     (View in source code to see train_dir example tree structure)\n",
      "\n",
      "     Structure:\n",
      "        <train_dir>/\n",
      "        ├── <person1>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   ├── <somename2>.jpeg\n",
      "        │   ├── ...\n",
      "        ├── <person2>/\n",
      "        │   ├── <somename1>.jpeg\n",
      "        │   └── <somename2>.jpeg\n",
      "        └── ...\n",
      "\n",
      "    :param model_save_path: (optional) path to save model on disk\n",
      "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
      "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
      "    :param verbose: verbosity of training\n",
      "    :return: returns knn classifier that was trained on the given data.\n",
      "    \"\"\"\n",
      "    X = []\n",
      "    y = []\n",
      "\n",
      "    # Loop through each person in the training set\n",
      "    for class_dir in os.listdir(train_dir):\n",
      "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
      "            continue\n",
      "\n",
      "        # Loop through each training image for the current person\n",
      "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
      "            image = face_recognition.load_image_file(img_path)\n",
      "            face_bounding_boxes = face_recognition.face_locations(image)\n",
      "\n",
      "            if len(face_bounding_boxes) != 1:\n",
      "                # If there are no people (or too many people) in a training image, skip the image.\n",
      "                if verbose:\n",
      "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
      "            else:\n",
      "                # Add face encoding for current image to the training set\n",
      "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
      "                y.append(class_dir)\n",
      "\n",
      "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
      "    if n_neighbors is None:\n",
      "        n_neighbors = int(round(math.sqrt(len(X))))\n",
      "        if verbose:\n",
      "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
      "\n",
      "    # Create and train the KNN classifier\n",
      "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
      "    knn_clf.fit(X, y)\n",
      "\n",
      "    # Save the trained KNN classifier\n",
      "    if model_save_path is not None:\n",
      "        with open(model_save_path, 'wb') as f:\n",
      "            pickle.dump(knn_clf, f)\n",
      "\n",
      "    return knn_clf\n",
      "\n",
      "<end of code>\n",
      "\n",
      "\n",
      "Given the above code enclosed within <start of code> and <end of code> tags, generate 3 questions based on it on possible edge cases that could be encountered. Then, answer the questions.\n",
      "\n",
      "3 Questions:\n",
      "1. What could be an edge case in the above code?\n",
      "2. How could the above code be modified to handle this edge case?\n",
      "3. What could be another edge case in the above code?\n",
      "\n",
      "Answer:\n",
      "1. Edge Case: The code assumes that there is exactly one face in each image. If an image contains multiple faces or no faces at all, the code will not work correctly.\n",
      "\n",
      "   Modification: The code could be modified to handle multiple faces in an image by modifying the face_recognition.face_locations() function call to return all face locations in an image. Then, the face_recognition.face_encodings() function could be called for each face location.\n",
      "\n",
      "   ```python\n",
      "   face_bounding_boxes = face_recognition.face_locations(image)\n",
      "   for face_location in face_bounding_boxes:\n"
     ]
    }
   ],
   "source": [
    "input_1 = \"\"\"<start of code> def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n",
    "    \\\"\"\"\n",
    "    Trains a k-nearest neighbors classifier for face recognition.\n",
    "\n",
    "    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n",
    "\n",
    "     (View in source code to see train_dir example tree structure)\n",
    "\n",
    "     Structure:\n",
    "        <train_dir>/\n",
    "        ├── <person1>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   ├── <somename2>.jpeg\n",
    "        │   ├── ...\n",
    "        ├── <person2>/\n",
    "        │   ├── <somename1>.jpeg\n",
    "        │   └── <somename2>.jpeg\n",
    "        └── ...\n",
    "\n",
    "    :param model_save_path: (optional) path to save model on disk\n",
    "    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n",
    "    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n",
    "    :param verbose: verbosity of training\n",
    "    :return: returns knn classifier that was trained on the given data.\n",
    "    \\\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Loop through each person in the training set\n",
    "    for class_dir in os.listdir(train_dir):\n",
    "        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
    "            continue\n",
    "\n",
    "        # Loop through each training image for the current person\n",
    "        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            face_bounding_boxes = face_recognition.face_locations(image)\n",
    "\n",
    "            if len(face_bounding_boxes) != 1:\n",
    "                # If there are no people (or too many people) in a training image, skip the image.\n",
    "                if verbose:\n",
    "                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n",
    "            else:\n",
    "                # Add face encoding for current image to the training set\n",
    "                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n",
    "                y.append(class_dir)\n",
    "\n",
    "    # Determine how many neighbors to use for weighting in the KNN classifier\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int(round(math.sqrt(len(X))))\n",
    "        if verbose:\n",
    "            print(\"Chose n_neighbors automatically:\", n_neighbors)\n",
    "\n",
    "    # Create and train the KNN classifier\n",
    "    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n",
    "    knn_clf.fit(X, y)\n",
    "\n",
    "    # Save the trained KNN classifier\n",
    "    if model_save_path is not None:\n",
    "        with open(model_save_path, 'wb') as f:\n",
    "            pickle.dump(knn_clf, f)\n",
    "\n",
    "    return knn_clf\n",
    "\n",
    "<end of code>\n",
    "\n",
    "\n",
    "Given the above code enclosed within <start of code> and <end of code> tags, generate 3 questions based on it on possible edge cases that could be encountered. Then, answer the questions.\"\"\"\n",
    "input = [input_1]\n",
    "result = pipe(input)\n",
    "print(result[0][0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "directed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
